[
  { "paragraph": " " },
  {
    "paragraph": " The researchers conducted experiments on the ImageNet dataset to evaluate the performance of deep residual nets. They found that deep residual nets are easier to optimize compared to plain nets, which simply stack layers. Increasing the depth of the residual nets resulted in improved accuracy, surpassing previous networks. The extremely deep residual nets achieved excellent results on the ImageNet classification dataset, with a top-5 error rate of 3.57%. The researchers won the 1st place in the ILSVRC 2015 classification competition with their 152-layer residual net. The residual learning principle is shown to be applicable in other recognition tasks, as the researchers also won 1st places in ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation competitions. The researchers suggest that the residual learning principle can be applied in other vision and non-vision problems. The concept of encoding residual vectors has been previously used in image recognition, vector quantization, and solving PDEs. Shortcut connections, similar to those used in the researchers' method, have been studied in the past, including in multi-layer perceptrons and highway networks. However, their method differs in that it always learns residual functions and does not use parameter-dependent gating functions.",
    "title": " Experiment and Results of Deep Residual Nets "
  },
  {
    "paragraph": " The authors conducted experiments to test the effectiveness of their proposed residual learning approach. They compared plain networks (inspired by VGG nets) with their counterpart residual networks. The plain networks had 34 layers and followed design rules for the number of filters and downsampling. The residual networks inserted shortcut connections to transform the plain networks into their residual versions. The shortcut connections could perform identity mapping or use a projection shortcut to match dimensions. The authors implemented their approach for ImageNet and compared the performance of the plain and residual networks. They observed consistent phenomena and provided instances for discussion. The plain network had fewer filters and lower complexity compared to VGG nets. The total number of weighted layers in the plain network was 34, while the residual network had additional shortcut connections. The authors measured the number of FLOPs (multiply-adds) for each network and found that the plain network had 18% of the FLOPs of VGG-19. The results of the experiments and comparisons were not explicitly mentioned in this context.",
    "title": " Experiment Design and Training Details "
  },
  {
    "paragraph": " The models were trained and tested using specific procedures. The image is resized with its shorter side randomly sampled in the range of 256 to 480 for scale augmentation. A 224x224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted. Batch normalization (BN) is used right after each convolution and before activation. The weights are initialized as in previous work and all plain/residual nets are trained from scratch. SGD with a mini-batch size of 256 is used as the optimizer. The learning rate starts from 0.1 and is divided by 10 when the error plateaus. The models are trained for up to 60,000 iterations. A weight decay of 0.0001 and a momentum of 0.9 are used. Dropout is not used in training. In testing, the standard 10-crop testing is adopted for comparison studies. The models are evaluated on the ImageNet 2012 classification dataset, which consists of 1000 classes. The models are trained on 1.28 million training images and evaluated on 50,000 validation images. The final result is obtained on the 100,000 test images reported by the test server. Both top-1 and top-5 error rates are evaluated.",
    "title": " Training and Testing Procedures "
  },
  {
    "paragraph": " Additional experiments and studies are conducted on the CIFAR-10 dataset, focusing on the behaviors of extremely deep networks. The network architecture consists of stacked layers with different filter sizes and subsampling performed by convolutions with a stride of 2. The network ends with global average pooling, a 10-way fully-connected layer, and softmax. The network has a total of 6n+2 stacked weighted layers. Shortcut connections are used, connected to pairs of 3x3 layers. Identity shortcuts are used in all cases. The models are trained with a weight decay of 0.0001 and momentum of 0.9, with no dropout. Mini-batch size of 128 is used on two GPUs. Learning rate starts at 0.1 and is divided by 10 at 32k and 48k iterations, training terminates at 64k iterations. Data augmentation is used during training, including padding and random cropping. Testing is done on the single view of the original 32x32 image. The performance of plain nets and ResNets is compared, showing that ResNets overcome optimization difficulties and exhibit accuracy gains with increased depth. A 110-layer ResNet is explored and found to converge well with adjusted learning rate. Layer response analysis shows that ResNets have generally smaller responses than plain counterparts, with deeper ResNets having even smaller magnitudes of responses. An aggressively deep model of over 1000 layers is explored, showing no optimization difficulty and achieving low training error.",
    "title": " Experiment and Results on CIFAR-10 Dataset ",
    "diagrams": {
      "type": "img",
      "figure": ["https://ar5iv.labs.arxiv.org/html/1512.03385/assets/x1.png"],
      "description": "Figure 1: Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer \u201cplain\u201d networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig.\u00a04."
    }
  }
]
